---
layout: post
title: 'Natural Language Video Localization'
author: Jinwoo Nam
date: 2020-09-09 22:00
tags: [Vision and Language]
image: /files/covers/nlvl.jpg
---



# Project Description

![Natural Language Video Localization](/files/images/nlvl/nlvl.gif)

NC소프트로부터 좋은 기회를 얻어 진행하게 된 프로젝트입니다. 

Natural Language Video Localization (NLVL)은 위의 영상과 같이 비디오와 자연어 쿼리가 주어졌을 때 비디오에서 쿼리가 가리키는 부분을 찾아내는 문제입니다. 다시 말해, 문제의 Input과 Output은 다음과 같습니다.

* Input
    * 랜덤한 길이의 비디오
    * 비디오의 한 부분을 가리키는 자연어 문장 (e.g. "A person is sitting at a table eating a sandwich")
* Output
    * 비디오에서 자연어가 가리키는 구간 정보 (e.g. "8s~13s")


# 프로젝트 목표

프로젝트는 NLVL을 위해 개발된 DiDeMo[1] 데이터셋에서 Stat-of-the-art 성능을 달성하는 것을 목표로 진행되었습니다. 이를 위해, 다음과 같은 세부 목표가 세워졌습니다.

1. 당시 가장 높은 성능을 보였던 ASST[2]를 재구현하고 성능 재현(Reproduce)
2. 재구현된 것을 개량하여 성능을 높임


# 개선 사항

> 프로젝트 규정상 자세한 내용을 공개할 수 없는 점 양해 부탁드립니다.

## Multi-Stream Learning

일반적으로 비디오를 다룰 때는 아래의 두 가지의 Feature를 사용합니다. 아래에서 L은 비디오의 길이, m과 n은 피쳐의 차원입니다.

* 각 프레임을 Resnet 등으로 인코딩한 RGB Feature (L\*m)
* 두 프레임 사이의 optical flow를 사용한 Flow Feature (L\*n)

저는 위의 두 가지 Feature에 더해, 추가로 2가지의 Feature를 더 만들어서 모델에 잇풋으로 주었습니다.

* Image Captioning Feature
    * 프레임마다 Image Captioning을 하고, 이것을 BERT[3]로 인코딩한 것을 video feature로 사용하였습니다.
    * 이러한 형태의 Feature는 salient한 object와 action을 단어의 형태로 explicit하게 제시해 줄 수 있어 자연어 쿼리와 쉽게 연관지을 수 있습니다.
    * 크기: L\*d<sub>BERT</sub>, d<sub>BERT</sub>는 BERT embedding의 차원
* Object Detection Feature
    * 프레임마다 top-k detection의 RoI Feature를 video feature로 사용하였습니다.
    * Image Captioning Feature는 salient object만을 잡아서 제시하는 반면, 이 Feature는 salient하지 않은 object도 제시해 줄 수 있습니다.  
    * 크기: L\*k\*d<sub>obj</sub>, k는 샘플링 된 오브젝트의 수, d<sub>obj</sub>는 object feature의 차원

## Feature-Wise Ensemble

현재까지의 NLVL 모델은 여러 종류의 Video Feature를 이용할 때 단순히 Feature들을 Concat하였습니다. 하지만 이러한 방식은 각 Feature별 특징을 고려하지 않고 model optimization을 하기 때문에, 각 Feature를 제대로 이용한다고 보기 어렵습니다.

따라서 개선된 모델에서는 각각의 video feature마다 작은 모델을 학습하고 이를 ensemble하는 방법을 사용하였습니다.


# 결과

> 프로젝트 규정상 자세한 내용을 공개할 수 없는 점 양해 부탁드립니다.

가장 중요하고 널리 사용되는 metric인 R@1(Top-1 Accuracy)에서 37.8으로 ASST[2]의 32.4보다 훨씬 높은 성능을 얻을 수 있었습니다.



# 후기

* 기계학습에서 유명한 격언으로 "악마는 디테일에 있다"라는 말이 있습니다. 이 말대로라면 비디오와 자연어의 조합은 분명 악마 중에서도 가장 사악한 종류일 것입니다.
* 데이터를 처리하기 위해 알아야 할 디테일(노하우)은 많았지만 관련된 정보를 얻을 방법은 극히 제한적이었습니다. 이 때 *타 연구자를 만나고 그들에게서 관련 노하우를 얻은 것*이 도움이 많이 되었습니다.
    * 프로젝트를 시작할 당시 연구실 내에 자연어나 비디오를 처리해 본 사람이 없었기 때문에 모든 것을 제가 직접 부딛혀 보고 알아내야 할 처지였습니다. 
    * 그러나, 혼자 모든 것을 시도해 보는 것은 현실적으로 어려웠기 때문에 *타 연구자를 만나고 그들에게서 관련 노하우를 얻어보려는*노력을 많이 했습니다. 몇몇 유명 학회(ICCV,KCCV)에 직접 참여하여 국내 유수 연구진들을 만나 비디오 처리에 관한 조언을 들었고, 우연한 기회로 버클리에서 근무하시는 한국인 연구자 분을 만나 자연어 처리에 관한 노하우를 얻었습니다.


---

[1] Hendricks et al. "Localizing Moments in Video with Temporal Language", ICCV, 2017

[2] Ning et al. "An Attentive Sequence to Sequence Translator for Localizing Video Clips by Natural Language", In IEEE Transactions on Multimedia, 2020

[3] Devlin et al. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", Arxiv preprint, 2018